{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TDeiovWHhHf",
        "outputId": "d0597f8a-3c39-42e0-a269-2140e1e96bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-3.15.2-py3-none-any.whl (271 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/271.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/271.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.1/271.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.15.2\n",
            "Collecting langchain[llms]\n",
            "  Downloading langchain-0.0.271-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain[llms])\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain[llms])\n",
            "  Downloading langsmith-0.0.26-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (2.2.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (8.2.3)\n",
            "Collecting clarifai>=9.1.0 (from langchain[llms])\n",
            "  Downloading clarifai-9.7.1-py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere<5,>=4 (from langchain[llms])\n",
            "  Downloading cohere-4.21-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub<1,>=0 (from langchain[llms])\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting manifest-ml<0.0.2,>=0.0.1 (from langchain[llms])\n",
            "  Downloading manifest_ml-0.0.1-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nlpcloud<2,>=1 (from langchain[llms])\n",
            "  Downloading nlpcloud-1.1.44-py3-none-any.whl (4.4 kB)\n",
            "Collecting openai<1,>=0 (from langchain[llms])\n",
            "  Downloading openai-0.27.9-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openlm<0.0.6,>=0.0.5 (from langchain[llms])\n",
            "  Downloading openlm-0.0.5-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (2.0.1+cu118)\n",
            "Collecting transformers<5,>=4 (from langchain[llms])\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.3.1)\n",
            "Collecting clarifai-grpc>=9.7.1 (from clarifai>=9.1.0->langchain[llms])\n",
            "  Downloading clarifai_grpc-9.7.3-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tritonclient==2.34.0 (from clarifai>=9.1.0->langchain[llms])\n",
            "  Downloading tritonclient-2.34.0-py3-none-manylinux1_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clarifai>=9.1.0->langchain[llms]) (23.1)\n",
            "Collecting python-rapidjson>=0.9.1 (from tritonclient==2.34.0->clarifai>=9.1.0->langchain[llms])\n",
            "  Downloading python_rapidjson-1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<3.0,>=2.0 (from cohere<5,>=4->langchain[llms])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere<5,>=4->langchain[llms])\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere<5,>=4->langchain[llms]) (6.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere<5,>=4->langchain[llms]) (2.0.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain[llms])\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain[llms])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[llms]) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[llms]) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[llms]) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[llms]) (4.7.1)\n",
            "Collecting dill>=0.3.5 (from manifest-ml<0.0.2,>=0.0.1->langchain[llms])\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting redis>=4.3.1 (from manifest-ml<0.0.2,>=0.0.1->langchain[llms])\n",
            "  Downloading redis-5.0.0-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.1/250.1 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlitedict>=2.0.0 (from manifest-ml<0.0.2,>=0.0.1->langchain[llms])\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain[llms]) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain[llms]) (2.6.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[llms]) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[llms]) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain[llms]) (2.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1->langchain[llms]) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1->langchain[llms]) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4->langchain[llms]) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5,>=4->langchain[llms])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5,>=4->langchain[llms])\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.44.0 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.7.1->clarifai>=9.1.0->langchain[llms]) (1.57.0)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.7.1->clarifai>=9.1.0->langchain[llms]) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.53.0 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.7.1->clarifai>=9.1.0->langchain[llms]) (1.60.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere<5,>=4->langchain[llms]) (3.16.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain[llms])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1->langchain[llms]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1->langchain[llms]) (1.3.0)\n",
            "Building wheels for collected packages: sqlitedict\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=096f92dbd162c01c56d60ad52d2950708ea9c43ed8446a67548607ccd22e8174\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built sqlitedict\n",
            "Installing collected packages: tokenizers, sqlitedict, safetensors, redis, python-rapidjson, mypy-extensions, marshmallow, fastavro, dill, backoff, typing-inspect, tritonclient, openlm, nlpcloud, manifest-ml, huggingface_hub, clarifai-grpc, transformers, openai, langsmith, dataclasses-json, cohere, clarifai, langchain\n",
            "Successfully installed backoff-2.2.1 clarifai-9.7.1 clarifai-grpc-9.7.3 cohere-4.21 dataclasses-json-0.5.14 dill-0.3.7 fastavro-1.8.2 huggingface_hub-0.16.4 langchain-0.0.271 langsmith-0.0.26 manifest-ml-0.0.1 marshmallow-3.20.1 mypy-extensions-1.0.0 nlpcloud-1.1.44 openai-0.27.9 openlm-0.0.5 python-rapidjson-1.10 redis-5.0.0 safetensors-0.3.2 sqlitedict-2.1.0 tokenizers-0.13.3 transformers-4.32.0 tritonclient-2.34.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf\n",
        "!pip install langchain[llms]\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WOK2taxIrjr",
        "outputId": "b8444d70-ced1-492d-96f1-51f143a32257"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnMHJfeR9MIB",
        "outputId": "3104de62-bfe4-4ce3-bd76-f3173bdfe528"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a long document we can split up.\n",
        "with open(\"/content/drive/My Drive/BeijingIntern/制度汇编2023年版-20230616(1).txt\") as f:\n",
        "    fptr = f.read()"
      ],
      "metadata": {
        "id": "NCfrUdpWJoRC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(fptr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ra-qsbQkQIf",
        "outputId": "1300e82a-4afa-47e7-e615-16fb6bb73659"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#helper function"
      ],
      "metadata": {
        "id": "iQWKjKLD8p2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "\n",
        "#nltkReadable = nltk.text.Text(jieba.cut(fptr))\n",
        "\n",
        "def recursive_text_split(original_text, chunk_size, max_depth, depth = 0):\n",
        "    if depth >= max_depth:\n",
        "      return [original_text]\n",
        "\n",
        "    text_splitter = NLTKTextSplitter(chunk_size=chunk_size)\n",
        "    chunks = text_splitter.split_text(original_text)\n",
        "\n",
        "    #print(depth)\n",
        "\n",
        "    if len(chunks) == 2:\n",
        "        return chunks\n",
        "\n",
        "    split_chunks = []\n",
        "    for chunk in chunks:\n",
        "        split_chunks.extend(recursive_text_split(chunk, chunk_size,max_depth,depth+1))\n",
        "\n",
        "    return split_chunks"
      ],
      "metadata": {
        "id": "kL9KSyjoNhcs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_list_to_files(data_list, file_prefix):\n",
        "    for index, item in enumerate(data_list):\n",
        "        filename = f\"{file_prefix}_{index}.txt\"\n",
        "        with open(filename, \"w\") as file:\n",
        "            file.write(item)"
      ],
      "metadata": {
        "id": "WhLjxMTOcaup"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nltk without split"
      ],
      "metadata": {
        "id": "MOXVc3WX8gwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#recursion\n",
        "final_split = recursive_text_split(fptr, 500,max_depth = 10)\n",
        "type(final_split)"
      ],
      "metadata": {
        "id": "dKg_ehr-Tw1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#non-recursion\n",
        "nltk_text_splitter = NLTKTextSplitter(chunk_size=500)\n",
        "final = nltk_text_splitter.split_text(fptr)\n",
        "len(final)"
      ],
      "metadata": {
        "id": "b1R0-E0Aavcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(final_split)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exQRrSsdanf3",
        "outputId": "f640807e-93f6-4bce-e56c-a794ac943b10"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "121"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#final[3]"
      ],
      "metadata": {
        "id": "xlNoBcsRbP7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_list_to_files(final,\"/content/drive/My Drive/BeijingIntern/separated/separated\")"
      ],
      "metadata": {
        "id": "FPIy4HIBcgPY"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text_splitter1 = NLTKTextSplitter(chunk_size=106000)\n",
        "#texts1 = text_splitter1.split_text(texts[0])\n",
        "#len(texts1)\n",
        "#type(texts1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g2j4HojWcM8",
        "outputId": "739ec48d-d2f8-4b51-8298-681c5cc220d9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#section for split then nltk"
      ],
      "metadata": {
        "id": "l0grmCO857ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####section for split then nltk\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 10000,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")\n",
        "\n",
        "#splitted_text = text_splitter.create_documents([fptr])\n",
        "\n",
        "nltk_text_splitter = NLTKTextSplitter(chunk_size=500)\n"
      ],
      "metadata": {
        "id": "rqQs4WeV51jD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_string(text, chunk_size):\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "    return chunks\n",
        ""
      ],
      "metadata": {
        "id": "i6zZorQ8_3CX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_chunks = split_string(fptr,5000)"
      ],
      "metadata": {
        "id": "lKTihYu-_4Hc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_and_nltk = []\n",
        "for i in range(len(split_chunks)):\n",
        "  split_and_nltk.extend(nltk_text_splitter.split_text(split_chunks[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G95K2hU9_C--",
        "outputId": "8f4c8461-7b30-4e4b-eae0-57064d6b5d02"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2309, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2742, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1688, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1092, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1939, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2798, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2884, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 798, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4757, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4014, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1685, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1949, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1042, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2044, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 903, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 3062, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1315, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2220, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 755, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2384, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 903, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1001, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1538, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 3271, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2421, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1726, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2082, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 704, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1011, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 683, which is longer than the specified 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_list_to_files(split_and_nltk,\"/content/drive/My Drive/BeijingIntern/split_and_nltk/separated\")"
      ],
      "metadata": {
        "id": "RfaEcHcMBHun"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splitted_text)\n",
        "type(splitted_text[0])\n",
        "len(split_chunks)\n",
        "type(split_chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRD1PKMq77jC",
        "outputId": "6091eaab-00be-419e-99e5-98b0842f083c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_wB8GWJ6ANE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}