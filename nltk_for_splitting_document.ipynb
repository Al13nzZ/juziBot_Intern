{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TDeiovWHhHf"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf\n",
        "!pip install langchain[llms]\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WOK2taxIrjr",
        "outputId": "b8444d70-ced1-492d-96f1-51f143a32257"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnMHJfeR9MIB",
        "outputId": "3104de62-bfe4-4ce3-bd76-f3173bdfe528"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a long document we can split up.\n",
        "with open(\"/content/drive/My Drive/BeijingIntern/制度汇编2023年版-20230616(1).txt\") as f:\n",
        "    fptr = f.read()"
      ],
      "metadata": {
        "id": "NCfrUdpWJoRC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(fptr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ra-qsbQkQIf",
        "outputId": "1300e82a-4afa-47e7-e615-16fb6bb73659"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#helper function"
      ],
      "metadata": {
        "id": "iQWKjKLD8p2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "\n",
        "#nltkReadable = nltk.text.Text(jieba.cut(fptr))\n",
        "\n",
        "def recursive_text_split(original_text, chunk_size, max_depth, depth = 0):\n",
        "    if depth >= max_depth:\n",
        "      return [original_text]\n",
        "\n",
        "    text_splitter = NLTKTextSplitter(chunk_size=chunk_size)\n",
        "    chunks = text_splitter.split_text(original_text)\n",
        "\n",
        "    #print(depth)\n",
        "\n",
        "    if len(chunks) == 2:\n",
        "        return chunks\n",
        "\n",
        "    split_chunks = []\n",
        "    for chunk in chunks:\n",
        "        split_chunks.extend(recursive_text_split(chunk, chunk_size,max_depth,depth+1))\n",
        "\n",
        "    return split_chunks"
      ],
      "metadata": {
        "id": "kL9KSyjoNhcs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_list_to_files(data_list, file_prefix):\n",
        "    for index, item in enumerate(data_list):\n",
        "        filename = f\"{file_prefix}_{index}.txt\"\n",
        "        with open(filename, \"w\") as file:\n",
        "            file.write(item)"
      ],
      "metadata": {
        "id": "WhLjxMTOcaup"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nltk without split"
      ],
      "metadata": {
        "id": "MOXVc3WX8gwe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FbtyIyylTdMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#recursion\n",
        "final_split = recursive_text_split(fptr, 500,max_depth = 10)\n",
        "type(final_split)"
      ],
      "metadata": {
        "id": "dKg_ehr-Tw1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#non-recursion\n",
        "nltk_text_splitter = NLTKTextSplitter(chunk_size=500)\n",
        "final = nltk_text_splitter.split_text(fptr)\n",
        "len(final)"
      ],
      "metadata": {
        "id": "b1R0-E0Aavcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(final_split)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exQRrSsdanf3",
        "outputId": "f640807e-93f6-4bce-e56c-a794ac943b10"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "121"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#final[3]"
      ],
      "metadata": {
        "id": "xlNoBcsRbP7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_list_to_files(final,\"/content/drive/My Drive/BeijingIntern/separated/separated\")"
      ],
      "metadata": {
        "id": "FPIy4HIBcgPY"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text_splitter1 = NLTKTextSplitter(chunk_size=106000)\n",
        "#texts1 = text_splitter1.split_text(texts[0])\n",
        "#len(texts1)\n",
        "#type(texts1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g2j4HojWcM8",
        "outputId": "739ec48d-d2f8-4b51-8298-681c5cc220d9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#section for split then nltk"
      ],
      "metadata": {
        "id": "l0grmCO857ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####section for split then nltk\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "#text_splitter = RecursiveCharacterTextSplitter(\n",
        " #   # Set a really small chunk size, just to show.\n",
        "  #  chunk_size = 5000,\n",
        "   # chunk_overlap  = 20,\n",
        "    #length_function = len,\n",
        "    #is_separator_regex = False,\n",
        "#)\n",
        "\n",
        "#splitted_text = text_splitter.create_documents([fptr])\n",
        "\n",
        "nltk_text_splitter = NLTKTextSplitter(chunk_size=500)\n"
      ],
      "metadata": {
        "id": "rqQs4WeV51jD"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_string(text, chunk_size):\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "    return chunks\n",
        ""
      ],
      "metadata": {
        "id": "i6zZorQ8_3CX"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_chunks = split_string(fptr,1000)"
      ],
      "metadata": {
        "id": "lKTihYu-_4Hc"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_and_nltk = []\n",
        "for i in range(len(split_chunks)):\n",
        "  split_and_nltk.extend(nltk_text_splitter.split_text(split_chunks[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G95K2hU9_C--",
        "outputId": "4e352501-df97-4abf-afc9-a0191aa52f61"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 742, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 688, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 939, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 798, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 884, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 798, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 757, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 685, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 949, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 550, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 755, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 903, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 540, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 704, which is longer than the specified 500\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 976, which is longer than the specified 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_list_to_files(split_and_nltk,\"/content/drive/My Drive/BeijingIntern/split_and_nltk/separated\")"
      ],
      "metadata": {
        "id": "RfaEcHcMBHun"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splitted_text)\n",
        "type(splitted_text[0])\n",
        "len(split_chunks)\n",
        "type(split_chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRD1PKMq77jC",
        "outputId": "6091eaab-00be-419e-99e5-98b0842f083c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "r-1G5SunhdBj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#txt to excel"
      ],
      "metadata": {
        "id": "nuWf5QqepAol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def txt_to_excel(txt_folder, excel_file):\n",
        "    data = []\n",
        "    for filename in os.listdir(txt_folder):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(txt_folder, filename), \"r\") as txt_file:\n",
        "                content = txt_file.read()\n",
        "                data.append(content)\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"NLTK\"])\n",
        "   # print(df)\n",
        "    df.to_excel(excel_file, index=False, startrow=0, startcol=2)  # Start writing from the third column\n"
      ],
      "metadata": {
        "id": "FvpO-3Juhfop"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_folder_path = \"/content/drive/My Drive/BeijingIntern/split_and_nltk/\"\n",
        "excel_file_path = \"/content/drive/My Drive/BeijingIntern/致远互联文本切分测试.xlsx\"\n",
        "txt_to_excel(txt_folder_path, excel_file_path)"
      ],
      "metadata": {
        "id": "vgF-0VCEiJCS"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file1_path = '/content/drive/My Drive/BeijingIntern/致远互联文本切分测试.xlsx'\n",
        "input_file2_path = '/content/drive/My Drive/BeijingIntern/致远互联文本切分测试 (1).xlsx'\n",
        "output_file_path = 'xxx'\n",
        "\n",
        "# Load Excel files\n",
        "df1 = pd.read_excel(input_file1_path)\n",
        "df2 = pd.read_excel(input_file2_path)\n",
        "\n",
        "# Concatenate dataframes\n",
        "merged_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
        "\n",
        "# Save the merged dataframe to an Excel file\n",
        "merged_df.to_excel(output_file_path, index=False)"
      ],
      "metadata": {
        "id": "JC6G1NywlrBi"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#to excel\n"
      ],
      "metadata": {
        "id": "eMTy0fPIpDv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_list_to_excel(data_list, excel_file):\n",
        "    df = pd.DataFrame(data_list, columns=[\"nltk\"])\n",
        "    df.to_excel(excel_file, index=False,startcol=2)\n",
        "\n",
        "# Example usage\n",
        "excel_file_path = \"/content/drive/My Drive/BeijingIntern/致远互联文本切分测试.xlsx\"  # Change the path as needed\n",
        "write_list_to_excel(split_and_nltk, excel_file_path)"
      ],
      "metadata": {
        "id": "s2tp7E4doAuJ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_excel(\"/content/drive/My Drive/BeijingIntern/致远互联文本切分测试.xlsx\")\n",
        "\n",
        "for i in range(len(split_and_nltk)):\n",
        "    df1.at[i, df1.columns[2]] = split_and_nltk[i]\n",
        "\n",
        "df1.to_excel(\"/content/drive/My Drive/BeijingIntern/致远互联文本切分测试.xlsx\", index=False,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf4X3H-CqFGW",
        "outputId": "d6895c3b-641f-4512-c887-1bb431b52c71"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       ﻿\\n\\n\\n目录\\n目录\\t1\\n第一卷 治理与信披系列制度\\t1\\n第一篇 公司治理\\t...\n",
            "1       附件三：《暂缓与豁免事项知情人保密承诺函》\\t177\\n2020.03.31总经理 《政府补...\n",
            "2       01.01总经理 《全面预算管理制度》\\t331\\n2018.08.01总经理 《财务管理制...\n",
            "3       08.02总经理 《银行账户管理办法》\\t507\\n2021.08.02总经理 《自有闲置资...\n",
            "4       .04.18总经理 《致远互联新人导师管理办法（试行）》\\t613\\n第六卷  企业管理综合...\n",
            "                              ...                        \n",
            "2290                                                  NaN\n",
            "2291                                                  NaN\n",
            "2292                                                  NaN\n",
            "2293                                                  NaN\n",
            "2294                                                  NaN\n",
            "Name: nltk - @黄毓烨, Length: 2295, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.to_excel(\"/content/drive/My Drive/BeijingIntern/致远互联文本切分测试.xlsx\", index=False,)"
      ],
      "metadata": {
        "id": "XsXvJ-iSsQtG"
      },
      "execution_count": 76,
      "outputs": []
    }
  ]
}